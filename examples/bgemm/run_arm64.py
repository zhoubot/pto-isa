#!/usr/bin/env python3
"""
PTO Example Runner - bgemm

Auto-generated by config_example.py
Configuration:
- Target Platform: arm64
- Binary Expansion: True
- Task Dump: True
- Task Graph PDF: True
- Benchmark Orchestration: Enabled (tasks/ms)
- Benchmark Runtime: Enabled (execution time)
- Sequence Length: 1024 - 4096 tokens (step: 1024)
- Accuracy Test: True
- Simulation: True
"""

import os
import sys
import time
import json
import subprocess
from datetime import datetime

# =============================================================================
# Path Setup
# =============================================================================

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
ROOT_DIR = os.path.dirname(os.path.dirname(SCRIPT_DIR))
SRC_DIR = os.path.join(ROOT_DIR, "src")
RUNTIME_DIR = os.path.join(SRC_DIR, "runtime")
OUTPUT_DIR = os.path.join(SCRIPT_DIR, "output")

sys.path.insert(0, SRC_DIR)

# =============================================================================
# Configuration
# =============================================================================

CONFIG = {
    "example_name": "bgemm",
    "target_platform": "arm64",
    "enable_binary_expansion": True,
    "enable_task_dump": True,
    "enable_task_graph_pdf": True,
    "benchmark_orchestration": True,  # tasks/ms without executing
    "benchmark_runtime": True,              # actual execution time
    "test_seq_len_min": 1024,
    "test_seq_len_max": 4096,
    "test_seq_len_step": 1024,
    "enable_accuracy_test": True,
    "enable_simulation": True,
    "num_warmup_iterations": 1,
    "num_benchmark_iterations": 1,
}

# =============================================================================
# Imports
# =============================================================================

try:
    from compile.pto_compile import (
        PTOFunctionBuilder, PTOModule, MultiBackendCodeGenerator,
        generate_arm64_code, generate_cuda_code, generate_ascend_code,
    )
    from isa_definition.pto_isa_definition import ElementType, MemorySpace
except ImportError as e:
    print(f"Error importing PTO modules: {e}")
    print("Make sure you're running from the correct directory.")
    sys.exit(1)

# =============================================================================
# Utility Functions
# =============================================================================

def print_header(title):
    print("\n" + "=" * 60)
    print(f"  {title}")
    print("=" * 60)


def ensure_dir(path):
    os.makedirs(path, exist_ok=True)
    return path


def run_command(cmd, cwd=None, timeout=300):
    """Run a shell command and return result."""
    try:
        result = subprocess.run(
            cmd, shell=True, cwd=cwd,
            capture_output=True, text=True, timeout=timeout
        )
        return result.returncode == 0, result.stdout, result.stderr
    except subprocess.TimeoutExpired:
        return False, "", "Command timed out"
    except Exception as e:
        return False, "", str(e)


# =============================================================================
# Code Generation
# =============================================================================

def generate_code():
    """Generate code for the target platform."""
    print_header("Code Generation")
    
    # Import the example module
    example_module_name = None
    for f in os.listdir(SCRIPT_DIR):
        if f.startswith('pto_') and f.endswith('.py') and f != 'run.py':
            example_module_name = f[:-3]
            break
    
    if not example_module_name:
        print("Error: No pto_*.py example file found!")
        return False
    
    print(f"  Loading example: {example_module_name}")
    
    # Import and run the example's main function
    sys.path.insert(0, SCRIPT_DIR)
    try:
        example_module = __import__(example_module_name)
        
        # Look for create_*_module functions first (for direct code generation)
        create_module_func = None
        for attr_name in dir(example_module):
            if attr_name.startswith('create_') and attr_name.endswith('_module'):
                create_module_func = getattr(example_module, attr_name)
                break
        
        if create_module_func is None and hasattr(example_module, 'create_module'):
            create_module_func = example_module.create_module
        
        # Always prefer create_*_module for code generation when available
        platform = CONFIG['target_platform']
        use_direct_generation = (create_module_func is not None)
        
        if use_direct_generation:
            print(f"  Creating module using {create_module_func.__name__}()...")
            module = create_module_func()
            
            # Generate code - put source files in generated_code/ subfolder
            platform_dir = ensure_dir(os.path.join(OUTPUT_DIR, platform))
            code_dir = ensure_dir(os.path.join(platform_dir, "generated_code"))
            
            gen = MultiBackendCodeGenerator(
                enable_fusion=True,
                analyze_buffers=True,
                module=module
            )
            
            for func_name, prog in module.functions.items():
                print(f"  Generating {platform} code for: {func_name}")
                
                if platform == "arm64":
                    code = gen.generate_arm64(prog)
                    ext = ".c"
                elif platform == "ascend_a2a3_sim":
                    code = gen.generate_ascend_a2a3_sim(prog)
                    ext = ".c"
                elif platform == "cuda":
                    code = gen.generate_cuda(prog)
                    ext = ".cu"
                else:  # ascend
                    code = gen.generate_ascend(prog)
                    ext = ".cpp"
                
                output_file = os.path.join(code_dir, f"{func_name}{ext}")
                with open(output_file, 'w') as f:
                    f.write(code)
                print(f"    -> {output_file}")
        elif hasattr(example_module, 'main'):
            print("  Running example main()...")
            example_module.main()
        else:
            print("  Warning: No main() or create_*_module() found, running module...")
            
    except Exception as e:
        print(f"  Error: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    print("  Code generation complete!")
    return True


# =============================================================================
# Compilation
# =============================================================================

def compile_code():
    """Compile generated code."""
    print_header("Compilation")
    
    platform = CONFIG['target_platform']
    platform_dir = os.path.join(OUTPUT_DIR, platform)
    code_dir = os.path.join(platform_dir, "generated_code")
    
    if not os.path.exists(code_dir):
        print(f"  No generated code found in {code_dir}")
        return False
    
    # Find orchestration file by checking:
    # 1. 'orchestration' in filename
    # 2. 'Function Type: Orchestration' in file content
    # 3. 'int main(' in file content (for simulator with main entry)
    orch_file = None
    for f in os.listdir(code_dir):
        if f.endswith('.c'):
            fpath = os.path.join(code_dir, f)
            if 'orchestration' in f.lower():
                orch_file = fpath
                break
            try:
                with open(fpath, 'r') as fp:
                    content = fp.read()
                    if 'Function Type: Orchestration' in content:
                        orch_file = fpath
                        break
                    if 'int main(' in content:
                        orch_file = fpath
                        break
            except:
                pass
    
    if not orch_file:
        print("  No orchestration file found to compile")
        return True  # Not an error, just no orchestration
    
    print(f"  Compiling: {os.path.basename(orch_file)}")
    
    # Build compile command - output executable to platform_dir (not code_dir)
    exe_basename = os.path.basename(orch_file).replace('.c', '')
    exe_path = os.path.join(platform_dir, exe_basename)
    
    compile_flags = ["-O2", "-std=c11"]
    if CONFIG['enable_binary_expansion']:
        compile_flags.append("-DPTO_BINARY_EXPANSION")
    if CONFIG['enable_task_dump']:
        compile_flags.append("-DPTO_TASK_DUMP")
    
    cmd = f"gcc {' '.join(compile_flags)} -I{RUNTIME_DIR} -o {exe_path} {orch_file} -lpthread"
    
    print(f"  Command: {cmd}")
    success, stdout, stderr = run_command(cmd, cwd=platform_dir)
    
    if success:
        print(f"  Compiled successfully: {exe_path}")
        return True
    else:
        print(f"  Compilation failed: {stderr}")
        return False


# =============================================================================
# Task Dump and Statistics
# =============================================================================

def run_task_dump():
    """Run and collect task dump statistics."""
    if not CONFIG['enable_task_dump']:
        return True
    
    print_header("Task Dump & Statistics")
    
    platform = CONFIG['target_platform']
    platform_dir = os.path.join(OUTPUT_DIR, platform)
    
    # Find executable
    exe_file = None
    for f in os.listdir(platform_dir):
        if not f.endswith(('.c', '.cu', '.cpp', '.txt', '.pdf', '.json', '.h')):
            exe_path = os.path.join(platform_dir, f)
            if os.access(exe_path, os.X_OK):
                exe_file = exe_path
                break
    
    if not exe_file:
        print("  No executable found")
        return False
    
    print(f"  Running: {os.path.basename(exe_file)}")
    success, stdout, stderr = run_command(exe_file, cwd=platform_dir, timeout=60)
    
    if success:
        print("  Execution successful")
        if stdout:
            print("  Output:")
            for line in stdout.split('\n')[:20]:
                print(f"    {line}")
        
        # Look for dump file
        dump_file = exe_file.replace('_orchestration', '_task_graph') + '.txt'
        if os.path.exists(dump_file):
            print(f"\n  Task graph dump: {dump_file}")
            analyze_task_dump(dump_file)
    else:
        print(f"  Execution failed: {stderr}")
    
    return success


def analyze_task_dump(dump_file):
    """Analyze task dump file and print statistics."""
    print("\n  Task Dump Statistics:")
    print("  " + "-" * 40)
    
    try:
        with open(dump_file, 'r') as f:
            content = f.read()
        
        # Count tasks
        task_count = content.count("Task ")
        print(f"    Total tasks: {task_count}")
        
        # Count by type if available
        lines = content.split('\n')
        task_types = {}
        for line in lines:
            if "func=" in line:
                # Extract function name
                start = line.find("func=") + 5
                end = line.find(",", start) if "," in line[start:] else len(line)
                func_name = line[start:end].strip('"')
                task_types[func_name] = task_types.get(func_name, 0) + 1
        
        if task_types:
            print("    Tasks by function:")
            for func, count in sorted(task_types.items(), key=lambda x: -x[1]):
                print(f"      {func}: {count}")
        
        # Dependency info
        dep_count = content.count("fanin=")
        print(f"    Dependencies tracked: {dep_count}")
        
    except Exception as e:
        print(f"    Error analyzing dump: {e}")


# =============================================================================
# Task Graph PDF Generation
# =============================================================================

def generate_task_graph_pdf():
    """Generate task graph visualization as PDF."""
    if not CONFIG['enable_task_graph_pdf']:
        return True
    
    print_header("Task Graph PDF Generation")
    
    # Check if graphviz is available
    success, _, _ = run_command("which dot")
    if not success:
        print("  Warning: graphviz not installed, skipping PDF generation")
        return True
    
    platform = CONFIG['target_platform']
    platform_dir = os.path.join(OUTPUT_DIR, platform)
    
    # Look for task graph txt file
    txt_file = None
    for f in os.listdir(platform_dir):
        if 'task_graph' in f and f.endswith('.txt'):
            txt_file = os.path.join(platform_dir, f)
            break
    
    if not txt_file:
        print("  No task graph file found")
        return True
    
    # Try to use visualize_taskgraph.py if available
    vis_script = os.path.join(ROOT_DIR, "visualize_taskgraph.py")
    if os.path.exists(vis_script):
        print(f"  Using visualize_taskgraph.py")
        cmd = f"python3 {vis_script} {txt_file}"
        success, stdout, stderr = run_command(cmd)
        if success:
            pdf_file = txt_file.replace('.txt', '.pdf')
            print(f"  Generated: {pdf_file}")
        else:
            print(f"  Warning: PDF generation failed: {stderr}")
    else:
        print("  visualize_taskgraph.py not found")
    
    return True


# =============================================================================
# Performance Benchmark
# =============================================================================

def run_performance_benchmark():
    """Run performance benchmarks based on configuration."""
    success = True
    
    # Run orchestration benchmark if enabled
    if CONFIG.get('benchmark_orchestration', False):
        if not run_orchestration_benchmark():
            success = False
    
    # Run runtime benchmark if enabled
    if CONFIG.get('benchmark_runtime', False):
        if not run_runtime_benchmark():
            success = False
    
    # If neither benchmark enabled, just return True
    if not CONFIG.get('benchmark_orchestration', False) and not CONFIG.get('benchmark_runtime', False):
        return True
    
    return success


def run_orchestration_benchmark():
    """Orchestration Benchmark - measures task submission throughput (tasks/ms) without executing."""
    print_header("Orchestration Benchmark")
    print("  Measuring task submission throughput (tasks/ms) without executing tasks")
    
    TILE_ROWS = 32  # Must match pto_llama7B_dynamic.py
    
    platform = CONFIG['target_platform']
    platform_dir = os.path.join(OUTPUT_DIR, platform)
    
    # Find executable
    exe_file = find_executable(platform_dir)
    if not exe_file:
        print("  No executable found for benchmarking")
        return False
    
    print(f"  Executable: {os.path.basename(exe_file)}")
    print(f"  Seq length: {CONFIG['test_seq_len_min']} - {CONFIG['test_seq_len_max']} tokens (step: {CONFIG['test_seq_len_step']})")
    print(f"  Iterations: {CONFIG['num_benchmark_iterations']}")
    
    all_results = []
    seq_lengths = list(range(CONFIG['test_seq_len_min'], CONFIG['test_seq_len_max'] + 1, CONFIG['test_seq_len_step']))
    
    print(f"\n  Testing {len(seq_lengths)} sequence lengths...\n")
    print("  " + "-" * 85)
    print(f"  {'seq_len':>10} | {'num_tiles':>10} | {'tasks':>10} | {'orch_time(ms)':>14} | {'tasks/ms':>12} | {'throughput':>15}")
    print("  " + "-" * 85)
    
    for seq_len in seq_lengths:
        num_tiles = seq_len // TILE_ROWS
        # Run with --benchmark-only flag (or environment variable)
        cmd = f"{exe_file} --benchmark-only 0 0 {num_tiles} 0"
        
        times = []
        tasks_submitted = 0
        tasks_per_ms_values = []
        
        for i in range(CONFIG['num_benchmark_iterations']):
            success, stdout, stderr = run_command(cmd, cwd=platform_dir, timeout=60)
            
            if success:
                import re
                # Parse BENCHMARK output: tasks=X time_ms=Y tasks_per_ms=Z
                match = re.search(r'BENCHMARK:.*tasks=(\d+)\s+time_ms=([\d.]+)\s+tasks_per_ms=([\d.]+)', stdout)
                if match:
                    tasks_submitted = int(match.group(1))
                    time_ms = float(match.group(2))
                    tpm = float(match.group(3))
                    times.append(time_ms)
                    tasks_per_ms_values.append(tpm)
        
        if times and tasks_per_ms_values:
            avg_time = sum(times) / len(times)
            avg_tpm = sum(tasks_per_ms_values) / len(tasks_per_ms_values)
            throughput = f"{avg_tpm * 1000:.0f} tasks/s"
            
            print(f"  {seq_len:>10} | {num_tiles:>10} | {tasks_submitted:>10} | {avg_time:>14.3f} | {avg_tpm:>12.2f} | {throughput:>15}")
            
            all_results.append({
                "seq_len": seq_len,
                "num_tiles": num_tiles,
                "tasks_submitted": tasks_submitted,
                "avg_time_ms": avg_time,
                "min_time_ms": min(times),
                "max_time_ms": max(times),
                "tasks_per_ms": avg_tpm,
                "tasks_per_sec": avg_tpm * 1000,
            })
        else:
            print(f"  {seq_len:>10} | {num_tiles:>10} | {'FAILED':>10} | {'-':>14} | {'-':>12} | {'-':>15}")
    
    print("  " + "-" * 85)
    
    if all_results:
        save_benchmark_results(platform_dir, "orchestration", all_results)
    
    return True


def run_runtime_benchmark():
    """Runtime Benchmark - measures actual execution/simulation time with workers."""
    print_header("Runtime Benchmark")
    print("  Measuring actual execution time with workers")
    
    TILE_ROWS = 32  # Must match pto_llama7B_dynamic.py
    
    platform = CONFIG['target_platform']
    platform_dir = os.path.join(OUTPUT_DIR, platform)
    
    # Find executable
    exe_file = find_executable(platform_dir)
    if not exe_file:
        print("  No executable found for benchmarking")
        return False
    
    print(f"  Executable: {os.path.basename(exe_file)}")
    print(f"  Seq length: {CONFIG['test_seq_len_min']} - {CONFIG['test_seq_len_max']} tokens (step: {CONFIG['test_seq_len_step']})")
    print(f"  Iterations: {CONFIG['num_benchmark_iterations']}")
    
    all_results = []
    seq_lengths = list(range(CONFIG['test_seq_len_min'], CONFIG['test_seq_len_max'] + 1, CONFIG['test_seq_len_step']))
    
    print(f"\n  Testing {len(seq_lengths)} sequence lengths...\n")
    print("  " + "-" * 65)
    print(f"  {'seq_len':>10} | {'num_tiles':>10} | {'tasks':>10} | {'exec_time(ms)':>14} | {'tasks/ms':>12}")
    print("  " + "-" * 65)
    
    for seq_len in seq_lengths:
        num_tiles = seq_len // TILE_ROWS
        # Run full execution (no --benchmark-only flag)
        cmd = f"{exe_file} 0 0 {num_tiles} 0"
        
        times = []
        tasks_submitted = 0
        
        for i in range(CONFIG['num_benchmark_iterations']):
            start = time.perf_counter()
            success, stdout, stderr = run_command(cmd, cwd=platform_dir, timeout=300)
            end = time.perf_counter()
            
            if success:
                elapsed_ms = (end - start) * 1000
                times.append(elapsed_ms)
                
                import re
                # Parse tasks submitted
                match = re.search(r'Submitted (\d+) tasks', stdout)
                if match:
                    tasks_submitted = int(match.group(1))
        
        if times:
            avg_time = sum(times) / len(times)
            tasks_per_ms = tasks_submitted / avg_time if avg_time > 0 else 0
            
            print(f"  {seq_len:>10} | {num_tiles:>10} | {tasks_submitted:>10} | {avg_time:>14.2f} | {tasks_per_ms:>12.2f}")
            
            all_results.append({
                "seq_len": seq_len,
                "num_tiles": num_tiles,
                "tasks_submitted": tasks_submitted,
                "avg_time_ms": avg_time,
                "min_time_ms": min(times),
                "max_time_ms": max(times),
                "tasks_per_ms": tasks_per_ms,
            })
        else:
            print(f"  {seq_len:>10} | {num_tiles:>10} | {'FAILED':>10} | {'-':>14} | {'-':>12}")
    
    print("  " + "-" * 65)
    
    if all_results:
        save_benchmark_results(platform_dir, "runtime", all_results)
    
    return True


def find_executable(platform_dir):
    """Find executable in platform directory."""
    for f in os.listdir(platform_dir):
        if f.endswith(('.c', '.cu', '.cpp', '.txt', '.pdf', '.json', '.h')):
            continue
        exe_path = os.path.join(platform_dir, f)
        if os.path.isfile(exe_path) and os.access(exe_path, os.X_OK):
            return exe_path
    return None


def save_benchmark_results(platform_dir, benchmark_type, all_results):
    """Save benchmark results and print summary."""
    if benchmark_type == "orchestration":
        avg_tpm = sum(r['tasks_per_ms'] for r in all_results) / len(all_results)
        max_tpm = max(r['tasks_per_ms'] for r in all_results)
        min_tpm = min(r['tasks_per_ms'] for r in all_results)
        
        print(f"\n  Summary:")
        print(f"    Average: {avg_tpm:.2f} tasks/ms ({avg_tpm * 1000:.0f} tasks/s)")
        print(f"    Peak:    {max_tpm:.2f} tasks/ms ({max_tpm * 1000:.0f} tasks/s)")
        print(f"    Min:     {min_tpm:.2f} tasks/ms ({min_tpm * 1000:.0f} tasks/s)")
        
        summary = {
            "avg_tasks_per_ms": avg_tpm,
            "max_tasks_per_ms": max_tpm,
            "min_tasks_per_ms": min_tpm,
        }
    else:
        avg_time = sum(r['avg_time_ms'] for r in all_results) / len(all_results)
        avg_tasks_per_ms = sum(r.get('tasks_per_ms', 0) for r in all_results) / len(all_results)
        
        print(f"\n  Summary:")
        print(f"    Average execution time: {avg_time:.2f} ms")
        print(f"    Average throughput: {avg_tasks_per_ms:.2f} tasks/ms")
        
        summary = {
            "avg_execution_time_ms": avg_time,
            "avg_tasks_per_ms": avg_tasks_per_ms,
        }
    
    results = {
        "timestamp": datetime.now().isoformat(),
        "benchmark_type": benchmark_type,
        "platform": CONFIG['target_platform'],
        "seq_len_range": {
            "min": CONFIG['test_seq_len_min'],
            "max": CONFIG['test_seq_len_max'],
            "step": CONFIG['test_seq_len_step']
        },
        "iterations": CONFIG['num_benchmark_iterations'],
        "results": all_results,
        "summary": summary,
    }
    
    results_file = os.path.join(platform_dir, f"benchmark_{benchmark_type}.json")
    with open(results_file, 'w') as f:
        json.dump(results, f, indent=2)
    print(f"\n  Results saved to: {results_file}")




# =============================================================================
# Accuracy Test
# =============================================================================

def run_accuracy_test():
    """Generate and run accuracy tests."""
    if not CONFIG['enable_accuracy_test']:
        return True
    
    print_header("Accuracy Test")
    print("  Accuracy testing not yet implemented")
    print("  (Requires reference implementation)")
    return True


# =============================================================================
# Simulation and Trace Generation
# =============================================================================

def run_simulation():
    """Run simulation and generate trace files."""
    if not CONFIG['enable_simulation']:
        return True
    
    print_header("Simulation & Trace Generation")
    
    if CONFIG['target_platform'] != 'ascend_a2a3_sim':
        print("  Simulation only available for ascend_a2a3_sim platform")
        return True
    
    platform_dir = os.path.join(OUTPUT_DIR, CONFIG['target_platform'])
    
    # Find simulation executable (any executable that's not a known non-executable extension)
    exe_file = None
    for f in os.listdir(platform_dir):
        # Skip directories, source files, and known output files
        if f.endswith(('.c', '.cu', '.cpp', '.txt', '.pdf', '.json', '.h')):
            continue
        exe_path = os.path.join(platform_dir, f)
        # Must be a file (not directory) and executable
        if os.path.isfile(exe_path) and os.access(exe_path, os.X_OK):
            exe_file = exe_path
            break
    
    if not exe_file:
        # Simulation already ran during task dump, so this is not a failure
        print("  Note: Simulation already completed during task dump step")
        return True
    
    # Calculate num_tiles for simulation using max seq_len from config
    TILE_ROWS = 32  # Must match pto_llama7B_dynamic.py
    sim_seq_len = CONFIG['test_seq_len_max']
    sim_num_tiles = sim_seq_len // TILE_ROWS
    
    print(f"  Running simulation: {os.path.basename(exe_file)}")
    print(f"  Simulation parameters: seq_len={sim_seq_len}, num_tiles={sim_num_tiles}")
    
    # Run with trace enabled and proper parameters
    env = os.environ.copy()
    env['PTO_TRACE_OUTPUT'] = os.path.join(platform_dir, 'trace.json')
    
    # Pass seq_len, tile_rows, num_tiles, zero as arguments
    sim_cmd = f"{exe_file} {sim_seq_len} {TILE_ROWS} {sim_num_tiles} 0"
    success, stdout, stderr = run_command(sim_cmd, cwd=platform_dir, timeout=120)
    
    if success:
        trace_file = os.path.join(platform_dir, 'trace.json')
        if os.path.exists(trace_file):
            print(f"  Trace file generated: {trace_file}")
            
            # Basic trace analysis
            try:
                with open(trace_file, 'r') as f:
                    trace_data = json.load(f)
                if isinstance(trace_data, list):
                    print(f"  Trace events: {len(trace_data)}")
            except:
                pass
        else:
            print("  Note: Trace file not generated (may need runtime support)")
    else:
        print(f"  Simulation failed: {stderr}")
    
    return success


# =============================================================================
# Main
# =============================================================================

def main():
    print_header(f"PTO Example Runner: {CONFIG['example_name']}")
    print(f"  Platform: {CONFIG['target_platform']}")
    print(f"  Output:   {OUTPUT_DIR}")
    
    ensure_dir(OUTPUT_DIR)
    
    steps = [
        ("Code Generation", generate_code),
        ("Compilation", compile_code),
        ("Task Dump", run_task_dump),
        ("Task Graph PDF", generate_task_graph_pdf),
        ("Performance Benchmark", run_performance_benchmark),
        ("Accuracy Test", run_accuracy_test),
        ("Simulation", run_simulation),
    ]
    
    results = []
    for name, func in steps:
        try:
            success = func()
            results.append((name, success))
        except Exception as e:
            print(f"  Error in {name}: {e}")
            results.append((name, False))
    
    print_header("Summary")
    for name, success in results:
        status = "✓ OK" if success else "✗ FAILED"
        print(f"  {name}: {status}")
    
    print("\nDone!")


if __name__ == "__main__":
    main()
