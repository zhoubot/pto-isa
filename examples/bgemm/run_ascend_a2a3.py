#!/usr/bin/env python3
"""
PTO Example Runner - bgemm

Auto-generated by config_example.py
Configuration:
- Target Platform: ascend_a2a3
- Binary Expansion: True
- Task Dump: True
- Task Graph PDF: True
- Benchmark Orchestration: Disabled (tasks/ms)
- Benchmark Runtime: Disabled (execution time)
- Sequence Length: 1024 - 16384 tokens (step: 1024)
- Accuracy Test: True
- Simulation: True
"""

import os
import sys
import time
import json
import shutil
import subprocess
from datetime import datetime

# =============================================================================
# Path Setup
# =============================================================================

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
ROOT_DIR = os.path.dirname(os.path.dirname(SCRIPT_DIR))
SRC_DIR = os.path.join(ROOT_DIR, "src")
RUNTIME_DIR = os.path.join(SRC_DIR, "runtime")
OUTPUT_DIR = os.path.join(SCRIPT_DIR, "output")

sys.path.insert(0, SRC_DIR)

# =============================================================================
# Configuration
# =============================================================================

CONFIG = {
    "example_name": "bgemm",
    "target_platform": "ascend_a2a3",
    "enable_binary_expansion": True,
    "enable_task_dump": True,
    "enable_task_graph_pdf": True,
    "benchmark_orchestration": False,  # tasks/ms without executing
    "benchmark_runtime": False,              # actual execution time
    "test_seq_len_min": 1024,
    "test_seq_len_max": 16384,
    "test_seq_len_step": 1024,
    "enable_accuracy_test": True,
    "enable_simulation": True,
    "num_warmup_iterations": 1,
    "num_benchmark_iterations": 1,
}

# =============================================================================
# Imports
# =============================================================================

try:
    from compile.pto_compile import (
        PTOFunctionBuilder, PTOModule, MultiBackendCodeGenerator,
        generate_arm64_code, generate_cuda_code, generate_ascend_code,
    )
    from isa_definition.pto_isa_definition import ElementType, MemorySpace
except ImportError as e:
    print(f"Error importing PTO modules: {e}")
    print("Make sure you're running from the correct directory.")
    sys.exit(1)

# =============================================================================
# Utility Functions
# =============================================================================

def print_header(title):
    print("\n" + "=" * 60)
    print(f"  {title}")
    print("=" * 60)


def ensure_dir(path):
    os.makedirs(path, exist_ok=True)
    return path


def run_command(cmd, cwd=None, timeout=300):
    """Run a shell command and return result."""
    try:
        result = subprocess.run(
            cmd, shell=True, cwd=cwd,
            capture_output=True, text=True, timeout=timeout
        )
        return result.returncode == 0, result.stdout, result.stderr
    except subprocess.TimeoutExpired:
        return False, "", "Command timed out"
    except Exception as e:
        return False, "", str(e)


# =============================================================================
# Code Generation
# =============================================================================

def generate_code():
    """Generate code for the target platform."""
    print_header("Code Generation")
    
    # Import the example module
    example_module_name = None
    for f in os.listdir(SCRIPT_DIR):
        if f.startswith('pto_') and f.endswith('.py') and f != 'run.py':
            example_module_name = f[:-3]
            break
    
    if not example_module_name:
        print("Error: No pto_*.py example file found!")
        return False
    
    print(f"  Loading example: {example_module_name}")
    
    # Import and run the example's main function
    sys.path.insert(0, SCRIPT_DIR)
    try:
        example_module = __import__(example_module_name)
        
        # Look for create_*_module functions first (for direct code generation)
        create_module_func = None
        for attr_name in dir(example_module):
            if attr_name.startswith('create_') and attr_name.endswith('_module'):
                create_module_func = getattr(example_module, attr_name)
                break
        
        if create_module_func is None and hasattr(example_module, 'create_module'):
            create_module_func = example_module.create_module
        
        # Always prefer create_*_module for code generation when available
        platform = CONFIG['target_platform']
        use_direct_generation = (create_module_func is not None)
        
        if use_direct_generation:
            print(f"  Creating module using {create_module_func.__name__}()...")
            module = create_module_func()
            
            # Clean output directory for fresh compilation
            platform_dir = os.path.join(OUTPUT_DIR, platform)
            if os.path.exists(platform_dir):
                print(f"  Cleaning output directory: {platform_dir}")
                shutil.rmtree(platform_dir)
            
            # Generate code - organize into subfolders for Ascend platforms
            platform_dir = ensure_dir(platform_dir)
            code_dir = ensure_dir(os.path.join(platform_dir, "generated_code"))
            
            # For Ascend A2A3 platforms, organize into subfolders
            is_a2a3_platform = platform in ("ascend_a2a3", "ascend_a2a3_sim")
            if is_a2a3_platform:
                orch_dir = ensure_dir(os.path.join(code_dir, "orchestration"))
                aic_dir = ensure_dir(os.path.join(code_dir, "incore_aic"))  # AI Core Cube
                aiv_dir = ensure_dir(os.path.join(code_dir, "incore_aiv"))  # AI Core Vector
            
            gen = MultiBackendCodeGenerator(
                enable_fusion=True,
                analyze_buffers=True,
                module=module
            )
            
            # Create PTO assembly compiler for generating .pto files
            from compile.pto_compile import PTOModuleCompiler
            pto_compiler = PTOModuleCompiler()
            
            for func_name, prog in module.functions.items():
                # Determine function type
                is_incore = getattr(prog, 'is_in_core', True)
                is_cube = getattr(prog, 'is_cube', False)
                
                if is_a2a3_platform:
                    if not is_incore:  # Orchestration function
                        func_type_str = "Orchestration"
                        target_dir = orch_dir
                    elif is_cube:  # InCore Cube function
                        func_type_str = "InCore Cube (AIC)"
                        target_dir = aic_dir
                    else:  # InCore Vector function
                        func_type_str = "InCore Vector (AIV)"
                        target_dir = aiv_dir
                    print(f"  Generating {platform} code for: {func_name} [{func_type_str}]")
                else:
                    target_dir = code_dir
                    print(f"  Generating {platform} code for: {func_name}")
                
                if platform == "arm64":
                    code = gen.generate_arm64(prog)
                    ext = ".c"
                elif platform == "ascend_a2a3_sim":
                    code = gen.generate_ascend_a2a3_sim(prog)
                    # InCore functions use C++ (PTO ISA API), orchestration uses C
                    ext = ".cpp" if is_incore else ".c"
                elif platform == "ascend_a2a3":
                    code = gen.generate_ascend_a2a3(prog)
                    ext = ".cpp" if is_incore else ".c"
                elif platform == "cuda":
                    code = gen.generate_cuda(prog)
                    ext = ".cu"
                else:  # other ascend platforms (a5, etc.)
                    code = gen.generate_ascend(prog)
                    ext = ".cpp"
                
                output_file = os.path.join(target_dir, f"{func_name}{ext}")
                with open(output_file, 'w') as f:
                    f.write(code)
                print(f"    -> {output_file}")
                
                # Generate .pto file for InCore functions (PTO assembly text format)
                if is_incore:
                    try:
                        pto_code = pto_compiler.compile_function(prog)
                        pto_file = os.path.join(target_dir, f"{func_name}.pto")
                        with open(pto_file, 'w') as f:
                            f.write(pto_code)
                        print(f"    -> {pto_file}")
                    except Exception as e:
                        print(f"    Warning: Could not generate .pto for {func_name}: {e}")
        elif hasattr(example_module, 'main'):
            print("  Running example main()...")
            example_module.main()
        else:
            print("  Warning: No main() or create_*_module() found, running module...")
            
    except Exception as e:
        print(f"  Error: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    print("  Code generation complete!")
    return True


# =============================================================================
# Compilation
# =============================================================================

def compile_code():
    """Compile generated code."""
    print_header("Compilation")
    
    platform = CONFIG['target_platform']
    platform_dir = os.path.join(OUTPUT_DIR, platform)
    code_dir = os.path.join(platform_dir, "generated_code")
    
    if not os.path.exists(code_dir):
        print(f"  No generated code found in {code_dir}")
        return False
    
    # Check for organized subfolder structure (A2A3 platforms)
    is_a2a3_platform = platform in ("ascend_a2a3", "ascend_a2a3_sim")
    
    # For ascend_a2a3 platform, compile to .so files
    if platform == "ascend_a2a3":
        return compile_ascend_a2a3(code_dir)
    
    # For other platforms, use the original compilation logic
    orch_dir = os.path.join(code_dir, "orchestration") if is_a2a3_platform else code_dir
    
    # Find orchestration file by checking:
    # 1. In orchestration/ subfolder (for A2A3)
    # 2. 'orchestration' in filename
    # 3. 'Function Type: Orchestration' in file content
    # 4. 'int main(' in file content (for simulator with main entry)
    orch_file = None
    search_dirs = [orch_dir] if is_a2a3_platform and os.path.exists(orch_dir) else [code_dir]
    
    for search_dir in search_dirs:
        if not os.path.exists(search_dir):
            continue
        for f in os.listdir(search_dir):
            if f.endswith('.c'):
                fpath = os.path.join(search_dir, f)
                if 'orchestration' in f.lower() or 'dynamic' in f.lower():
                    orch_file = fpath
                    break
                try:
                    with open(fpath, 'r') as fp:
                        content = fp.read()
                        if 'Function Type: Orchestration' in content:
                            orch_file = fpath
                            break
                        if 'int main(' in content:
                            orch_file = fpath
                            break
                except:
                    pass
        if orch_file:
            break
    
    if not orch_file:
        print("  No orchestration file found to compile")
        return True  # Not an error, just no orchestration
    
    print(f"  Compiling: {os.path.basename(orch_file)}")
    
    # Build compile command - output executable to platform_dir (not code_dir)
    exe_basename = os.path.basename(orch_file).replace('.c', '')
    exe_path = os.path.join(platform_dir, exe_basename)
    
    compile_flags = ["-O2", "-std=c11"]
    if CONFIG['enable_binary_expansion']:
        compile_flags.append("-DPTO_BINARY_EXPANSION")
    if CONFIG['enable_task_dump']:
        compile_flags.append("-DPTO_TASK_DUMP")
    
    # Add include paths
    include_paths = [f"-I{RUNTIME_DIR}"]
    if is_a2a3_platform:
        # Add InCore directories to include path for InCore function headers
        aic_dir = os.path.join(code_dir, "incore_aic")
        aiv_dir = os.path.join(code_dir, "incore_aiv")
        if os.path.exists(aic_dir):
            include_paths.append(f"-I{aic_dir}")
        if os.path.exists(aiv_dir):
            include_paths.append(f"-I{aiv_dir}")
        include_paths.append(f"-I{code_dir}")
    
    cmd = f"gcc {' '.join(compile_flags)} {' '.join(include_paths)} -o {exe_path} {orch_file} -lpthread"
    
    print(f"  Command: {cmd}")
    success, stdout, stderr = run_command(cmd, cwd=platform_dir)
    
    if success:
        print(f"  Compiled successfully: {exe_path}")
        return True
    else:
        print(f"  Compilation failed: {stderr}")
        return False


def generate_test_program_template(code_dir, example_name):
    """
    Generate a test program template for A2A3 runtime entry.
    
    This creates a C file that:
    1. Initializes the A2A3 runtime with configuration
    2. Sets up host memory buffers
    3. Copies data to device (copyToDevice)
    4. Executes the orchestration function
    5. Copies results back (copyFromDevice)
    6. Cleans up the runtime
    """
    # Use double-quoted triple string with escaped braces for C code
    test_program = """/**
 * PTO Runtime Test Program - """ + example_name + """
 * 
 * Auto-generated test entry point for A2A3 Runtime.
 * This program demonstrates the complete runtime workflow:
 * 1. Load orchestration and InCore .so files
 * 2. Initialize runtime with thread configuration
 * 3. Transfer data to device
 * 4. Execute orchestration function
 * 5. Transfer results back to host
 * 6. Clean up resources
 * 
 * Thread Configuration:
 * - 1 Orchestration AICPU thread
 * - 3 Dependency Resolution AICPU threads
 * - 48 AIV (Vector) workers
 * - 24 AIC (Cube) workers
 */

#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <time.h>

// Include A2A3 Runtime API
#include "runtime_a2a3/a2a3_runtime_api.h"

// =============================================================================
// Test Configuration
// =============================================================================

#define TEST_INPUT_SIZE   (1024 * 1024 * sizeof(float))  // 1M floats
#define TEST_OUTPUT_SIZE  (1024 * 1024 * sizeof(float))  // 1M floats

// =============================================================================
// Helper Functions
// =============================================================================

static double get_time_ms(void) {
    struct timespec ts;
    clock_gettime(CLOCK_MONOTONIC, &ts);
    return ts.tv_sec * 1000.0 + ts.tv_nsec / 1000000.0;
}

static void init_test_data(float* data, size_t count) {
    for (size_t i = 0; i < count; i++) {
        data[i] = (float)(i % 1000) / 1000.0f;
    }
}

static int verify_results(const float* expected, const float* actual, size_t count) {
    int errors = 0;
    const float epsilon = 1e-5f;
    for (size_t i = 0; i < count && errors < 10; i++) {
        float diff = expected[i] - actual[i];
        if (diff < 0) diff = -diff;
        if (diff > epsilon) {
            printf("  Mismatch at [%zu]: expected %.6f, got %.6f\\n", 
                   i, expected[i], actual[i]);
            errors++;
        }
    }
    return errors;
}

// =============================================================================
// Main Entry Point
// =============================================================================

int main(int argc, char** argv) {
    printf("=======================================================\\n");
    printf("  PTO A2A3 Runtime Test: """ + example_name + """\\n");
    printf("=======================================================\\n\\n");
    
    int ret;
    double start_time, end_time;
    
    // =========================================================================
    // 1. Configure Runtime
    // =========================================================================
    printf("[1/6] Configuring runtime...\\n");
    
    A2A3RuntimeConfig config;
    a2a3_config_init_defaults(&config);
    
    // Set paths to compiled .so files
    config.orchestration_so_path = "generated_code/orchestration/lib_orchestration.so";
    config.orchestration_func_name = """ + '"' + example_name + """_dynamic";  // Orchestration function name
    config.incore_aiv_dir = "generated_code/incore_aiv/";
    config.incore_aic_dir = "generated_code/incore_aic/";
    
    // Thread configuration (as specified in requirements)
    config.num_orch_threads = 1;    // 1 Orchestration AICPU thread
    config.num_dep_threads = 3;     // 3 Dependency Resolution threads
    config.num_aiv_workers = 48;    // 48 AIV (Vector) workers
    config.num_aic_workers = 24;    // 24 AIC (Cube) workers
    
    config.debug_enabled = true;
    
    // DEBUG_ORCHESTRATION mode: Only run orchestration, skip task execution
    // Set to false to test full execution with workers
    config.debug_orchestration_only = false;  // Changed to false for full execution test
    
    printf("  Orchestration SO: %s\\n", config.orchestration_so_path);
    printf("  InCore AIV dir:   %s\\n", config.incore_aiv_dir);
    printf("  InCore AIC dir:   %s\\n", config.incore_aic_dir);
    printf("  Orch threads:     %d\\n", config.num_orch_threads);
    printf("  Dep threads:      %d\\n", config.num_dep_threads);
    printf("  AIV workers:      %d\\n", config.num_aiv_workers);
    printf("  AIC workers:      %d\\n", config.num_aic_workers);
    
    // =========================================================================
    // 2. Initialize Runtime
    // =========================================================================
    printf("\\n[2/6] Initializing runtime...\\n");
    start_time = get_time_ms();
    
    ret = a2a3_runtime_init(&config);
    if (ret != A2A3_SUCCESS) {
        fprintf(stderr, "ERROR: Failed to initialize runtime: %s\\n",
                a2a3_runtime_error_string(ret));
        return 1;
    }
    
    end_time = get_time_ms();
    printf("  Runtime initialized in %.2f ms\\n", end_time - start_time);
    
    // =========================================================================
    // 3. Allocate and Initialize Host Buffers
    // =========================================================================
    printf("\\n[3/6] Allocating host buffers...\\n");
    
    float* host_input = (float*)malloc(TEST_INPUT_SIZE);
    float* host_output = (float*)malloc(TEST_OUTPUT_SIZE);
    float* host_expected = (float*)malloc(TEST_OUTPUT_SIZE);  // For verification
    
    if (!host_input || !host_output || !host_expected) {
        fprintf(stderr, "ERROR: Failed to allocate host buffers\\n");
        a2a3_runtime_finalize();
        return 1;
    }
    
    // Initialize input data
    init_test_data(host_input, TEST_INPUT_SIZE / sizeof(float));
    memset(host_output, 0, TEST_OUTPUT_SIZE);
    
    printf("  Input buffer:  %zu bytes\\n", (size_t)TEST_INPUT_SIZE);
    printf("  Output buffer: %zu bytes\\n", (size_t)TEST_OUTPUT_SIZE);
    
    // =========================================================================
    // 4. Copy Data to Device (copyToDevice)
    // =========================================================================
    printf("\\n[4/6] Copying data to device...\\n");
    start_time = get_time_ms();
    
    // Allocate device buffers
    void* dev_input = a2a3_runtime_malloc(TEST_INPUT_SIZE);
    void* dev_output = a2a3_runtime_malloc(TEST_OUTPUT_SIZE);
    
    if (!dev_input || !dev_output) {
        fprintf(stderr, "ERROR: Failed to allocate device buffers\\n");
        free(host_input);
        free(host_output);
        free(host_expected);
        a2a3_runtime_finalize();
        return 1;
    }
    
    // Copy input to device
    ret = a2a3_runtime_copy_to_device(dev_input, host_input, TEST_INPUT_SIZE);
    if (ret != A2A3_SUCCESS) {
        fprintf(stderr, "ERROR: copyToDevice failed: %s\\n",
                a2a3_runtime_error_string(ret));
        a2a3_runtime_free(dev_input);
        a2a3_runtime_free(dev_output);
        free(host_input);
        free(host_output);
        free(host_expected);
        a2a3_runtime_finalize();
        return 1;
    }
    
    end_time = get_time_ms();
    printf("  Data copied to device in %.2f ms\\n", end_time - start_time);
    
    // =========================================================================
    // 5. Execute Orchestration Function
    // =========================================================================
    printf("\\n[5/6] Executing orchestration function...\\n");
    start_time = get_time_ms();
    
    // Allocate additional device buffers for intermediate results
    // For bgemm: A, B, C, P0, P1, P2 (6 matrix buffers)
    size_t matrix_size = 64 * 128 * sizeof(float);  // Default tile size
    void* dev_P0 = a2a3_runtime_malloc(TEST_OUTPUT_SIZE);
    void* dev_P1 = a2a3_runtime_malloc(TEST_OUTPUT_SIZE);
    void* dev_P2 = a2a3_runtime_malloc(TEST_OUTPUT_SIZE);
    
    // Set up scalar parameters
    int32_t seq_len = 64;
    int32_t tile_rows = 8;
    int32_t num_tiles = 8;
    float zero_val = 0.0f;
    
    // Create void** array to pass parameters to orchestration function
    // Order must match orchestration function's parameter extraction
    void* user_data[16];
    user_data[0] = dev_input;    // A matrix
    user_data[1] = (char*)dev_input + TEST_INPUT_SIZE/2;  // B matrix (second half of input)
    user_data[2] = dev_output;   // C matrix (output)
    user_data[3] = dev_P0;       // P0 intermediate
    user_data[4] = dev_P1;       // P1 intermediate
    user_data[5] = dev_P2;       // P2 intermediate
    user_data[6] = &seq_len;     // seq_len scalar
    user_data[7] = &tile_rows;   // tile_rows scalar
    user_data[8] = &num_tiles;   // num_tiles scalar
    user_data[9] = &zero_val;    // zero scalar
    
    printf("  Params: seq_len=%d, tile_rows=%d, num_tiles=%d\\n", seq_len, tile_rows, num_tiles);
    
    ret = a2a3_runtime_execute(user_data);
    if (ret != A2A3_SUCCESS) {
        fprintf(stderr, "ERROR: Execution failed: %s\\n",
                a2a3_runtime_error_string(ret));
        a2a3_runtime_free(dev_input);
        a2a3_runtime_free(dev_output);
        a2a3_runtime_free(dev_P0);
        a2a3_runtime_free(dev_P1);
        a2a3_runtime_free(dev_P2);
        free(host_input);
        free(host_output);
        free(host_expected);
        a2a3_runtime_finalize();
        return 1;
    }
    
    end_time = get_time_ms();
    printf("  Execution completed in %.2f ms\\n", end_time - start_time);
    
    // =========================================================================
    // 6. Copy Results from Device (copyFromDevice)
    // =========================================================================
    printf("\\n[6/6] Copying results from device...\\n");
    start_time = get_time_ms();
    
    ret = a2a3_runtime_copy_from_device(host_output, dev_output, TEST_OUTPUT_SIZE);
    if (ret != A2A3_SUCCESS) {
        fprintf(stderr, "ERROR: copyFromDevice failed: %s\\n",
                a2a3_runtime_error_string(ret));
    }
    
    end_time = get_time_ms();
    printf("  Data copied from device in %.2f ms\\n", end_time - start_time);
    
    // =========================================================================
    // Print Statistics
    // =========================================================================
    printf("\\n");
    a2a3_runtime_print_stats();
    
    // =========================================================================
    // Save Data for Python Accuracy Verification
    // =========================================================================
    printf("\\nSaving data for accuracy verification...\\n");
    
    FILE* f_input = fopen("accuracy_input.bin", "wb");
    FILE* f_output = fopen("accuracy_output.bin", "wb");
    FILE* f_params = fopen("accuracy_params.txt", "w");
    
    if (f_input && f_output && f_params) {
        fwrite(host_input, 1, TEST_INPUT_SIZE, f_input);
        fwrite(host_output, 1, TEST_OUTPUT_SIZE, f_output);
        fprintf(f_params, "input_size=%zu\\n", (size_t)TEST_INPUT_SIZE);
        fprintf(f_params, "output_size=%zu\\n", (size_t)TEST_OUTPUT_SIZE);
        fprintf(f_params, "num_tiles=%d\\n", num_tiles);
        fprintf(f_params, "tile_m=64\\n");
        fprintf(f_params, "tile_n=128\\n");
        fprintf(f_params, "tile_k=64\\n");
        fprintf(f_params, "k_tiles=8\\n");
        printf("  Saved: accuracy_input.bin, accuracy_output.bin, accuracy_params.txt\\n");
    } else {
        printf("  WARNING: Could not save accuracy data files\\n");
    }
    
    if (f_input) fclose(f_input);
    if (f_output) fclose(f_output);
    if (f_params) fclose(f_params);
    
    // =========================================================================
    // Cleanup
    // =========================================================================
    printf("\\nCleaning up...\\n");
    
    a2a3_runtime_free(dev_input);
    a2a3_runtime_free(dev_output);
    a2a3_runtime_free(dev_P0);
    a2a3_runtime_free(dev_P1);
    a2a3_runtime_free(dev_P2);
    free(host_input);
    free(host_output);
    free(host_expected);
    
    a2a3_runtime_finalize();
    
    printf("\\nTest completed successfully!\\n");
    return 0;
}
"""
    
    # Write the test program
    test_file = os.path.join(code_dir, "test_program.c")
    with open(test_file, 'w') as f:
        f.write(test_program)
    
    return test_file


def compile_ascend_a2a3(code_dir):
    """
    Compile generated code for Ascend A2/A3 platform.
    
    Compiles:
    - orchestration/*.c → lib_orchestration.so (shared library)
    - incore_aic/*.cpp → *.o (AICore Cube object files)
    - incore_aiv/*.cpp → *.o (AICore Vector object files)
    - test_program.c → test_program (executable)
    
    Output files are saved in the same folder as source files.
    """
    import glob
    
    # Get ASCEND_HOME_PATH for toolchain
    ascend_home = os.environ.get('ASCEND_HOME_PATH', '/usr/local/Ascend/ascend-toolkit/latest')
    ccec_path = os.path.join(ascend_home, 'bin', 'ccec')
    ld_path = os.path.join(ascend_home, 'bin', 'ld.lld')
    
    # Check if ccec compiler exists
    if not os.path.exists(ccec_path):
        print(f"  Warning: ccec compiler not found at {ccec_path}")
        print(f"  Please set ASCEND_HOME_PATH environment variable")
        print(f"  Skipping AICore kernel compilation")
        ccec_available = False
    else:
        ccec_available = True
    
    success = True
    
    # Directory paths
    orch_dir = os.path.join(code_dir, "orchestration")
    aic_dir = os.path.join(code_dir, "incore_aic")
    aiv_dir = os.path.join(code_dir, "incore_aiv")
    
    # Check if CANN SDK is available
    cann_available = os.path.exists(os.path.join(ascend_home, 'include', 'acl', 'acl.h'))
    if not cann_available:
        print(f"  Note: CANN SDK not found, using stub compilation mode")
        print(f"        (Define CANN_SDK_AVAILABLE for full hardware support)")
    
    # 1. Compile orchestration functions to shared library
    if os.path.exists(orch_dir):
        print("\n  [1/4] Compiling orchestration functions...")
        c_files = glob.glob(os.path.join(orch_dir, "*.c"))
        if c_files:
            # Compile all .c files to a shared library
            so_path = os.path.join(orch_dir, "lib_orchestration.so")
            
            compile_flags = ["-O2", "-std=c11", "-fPIC", "-shared", "-D_POSIX_C_SOURCE=199309L"]
            if CONFIG['enable_binary_expansion']:
                compile_flags.append("-DPTO_BINARY_EXPANSION")
            if CONFIG['enable_task_dump']:
                compile_flags.append("-DPTO_TASK_DUMP")
            
            # Add CANN SDK related flags
            if cann_available:
                compile_flags.append("-DCANN_SDK_AVAILABLE")
            else:
                # Skip CANN check for stub compilation
                compile_flags.append("-DA2A3_SKIP_CANN_CHECK")
            
            include_paths = [
                f"-I{RUNTIME_DIR}",
                f"-I{code_dir}",
                f"-I{aic_dir}" if os.path.exists(aic_dir) else "",
                f"-I{aiv_dir}" if os.path.exists(aiv_dir) else "",
            ]
            include_paths = [p for p in include_paths if p]  # Remove empty
            
            # Add CANN SDK include path if available
            if cann_available:
                include_paths.append(f"-I{ascend_home}/include")
            
            src_files = " ".join(c_files)
            cmd = f"gcc {' '.join(compile_flags)} {' '.join(include_paths)} -o {so_path} {src_files} -lpthread"
            
            print(f"    Command: {cmd}")
            ok, stdout, stderr = run_command(cmd, cwd=orch_dir, timeout=120)
            
            if ok:
                print(f"    ✓ Compiled: {so_path}")
            else:
                print(f"    ✗ Failed: {stderr}")
                success = False
        else:
            print("    No .c files found in orchestration/")
    else:
        print("\n  [1/4] Skipping orchestration (no orchestration/ directory)")
    
    # 2. Compile InCore AIC (AI Core Cube) functions
    if os.path.exists(aic_dir) and ccec_available:
        print("\n  [2/4] Compiling InCore AIC (Cube) functions...")
        cpp_files = glob.glob(os.path.join(aic_dir, "*.cpp"))
        if cpp_files:
            for cpp_file in cpp_files:
                basename = os.path.basename(cpp_file).replace('.cpp', '')
                obj_path = os.path.join(aic_dir, f"{basename}.o")
                
                # Compile for AIC (Cube) architecture
                cmd = (
                    f"{ccec_path} -c -O3 -x cce -std=c++17 "
                    f"--cce-aicore-only --cce-aicore-arch=dav-c220-cube "
                    f"-D__AIC__ -DMEMORY_BASE "
                    f"-mllvm -cce-aicore-stack-size=0x8000 "
                    f"-mllvm -cce-aicore-function-stack-size=0x8000 "
                    f"-I{ROOT_DIR}/include "
                    f"-I{code_dir} -I{aic_dir} "
                    f"-o {obj_path} {cpp_file}"
                )
                
                print(f"    Compiling {basename}.cpp for AIC...")
                ok, stdout, stderr = run_command(cmd, cwd=aic_dir, timeout=120)
                
                if ok:
                    print(f"    ✓ Compiled: {obj_path}")
                else:
                    print(f"    ✗ Failed: {stderr}")
                    success = False
        else:
            print("    No .cpp files found in incore_aic/")
    elif not ccec_available:
        print("\n  [2/4] Skipping InCore AIC (ccec compiler not available)")
    else:
        print("\n  [2/4] Skipping InCore AIC (no incore_aic/ directory)")
    
    # 3. Compile InCore AIV (AI Core Vector) functions
    if os.path.exists(aiv_dir) and ccec_available:
        print("\n  [3/4] Compiling InCore AIV (Vector) functions...")
        cpp_files = glob.glob(os.path.join(aiv_dir, "*.cpp"))
        if cpp_files:
            for cpp_file in cpp_files:
                basename = os.path.basename(cpp_file).replace('.cpp', '')
                obj_path = os.path.join(aiv_dir, f"{basename}.o")
                
                # Compile for AIV (Vector) architecture
                cmd = (
                    f"{ccec_path} -c -O3 -x cce -std=c++17 "
                    f"--cce-aicore-only --cce-aicore-arch=dav-c220-vec "
                    f"-D__AIV__ -DMEMORY_BASE "
                    f"-mllvm -cce-aicore-stack-size=0x8000 "
                    f"-mllvm -cce-aicore-function-stack-size=0x8000 "
                    f"-I{ROOT_DIR}/include "
                    f"-I{code_dir} -I{aiv_dir} "
                    f"-o {obj_path} {cpp_file}"
                )
                
                print(f"    Compiling {basename}.cpp for AIV...")
                ok, stdout, stderr = run_command(cmd, cwd=aiv_dir, timeout=120)
                
                if ok:
                    print(f"    ✓ Compiled: {obj_path}")
                else:
                    print(f"    ✗ Failed: {stderr}")
                    success = False
        else:
            print("    No .cpp files found in incore_aiv/")
    elif not ccec_available:
        print("\n  [3/4] Skipping InCore AIV (ccec compiler not available)")
    else:
        print("\n  [3/4] Skipping InCore AIV (no incore_aiv/ directory)")
    
    # 4. Generate and compile test program
    print("\n  [4/4] Generating and compiling test program...")
    
    # Get example name from directory structure (e.g., "bgemm" from .../bgemm/output/platform/generated_code)
    platform_dir = os.path.dirname(code_dir)  # .../bgemm/output/platform
    output_dir = os.path.dirname(platform_dir)  # .../bgemm/output
    example_dir = os.path.dirname(output_dir)  # .../bgemm
    example_name = os.path.basename(example_dir)  # bgemm
    test_file = generate_test_program_template(code_dir, example_name)
    print(f"    Generated: {test_file}")
    
    # Get parent directory (platform_dir) for test executable
    platform_dir = os.path.dirname(code_dir)
    test_exe = os.path.join(platform_dir, "test_program")
    
    compile_flags = ["-O2", "-std=c11", "-D_POSIX_C_SOURCE=199309L"]
    if cann_available:
        compile_flags.append("-DCANN_SDK_AVAILABLE")
    else:
        compile_flags.append("-DA2A3_SKIP_CANN_CHECK")
    
    include_paths = [
        f"-I{RUNTIME_DIR}",
        f"-I{code_dir}",
    ]
    
    # Add CANN SDK include path if available
    if cann_available:
        include_paths.append(f"-I{ascend_home}/include")
    
    # Find all runtime source files needed
    runtime_sources = [
        os.path.join(RUNTIME_DIR, "pto_runtime.c"),
        os.path.join(RUNTIME_DIR, "runtime_a2a3", "a2a3_runtime.c"),
        os.path.join(RUNTIME_DIR, "runtime_a2a3", "host", "a2a3_host.c"),
        os.path.join(RUNTIME_DIR, "runtime_a2a3", "host", "a2a3_so_loader.c"),
        os.path.join(RUNTIME_DIR, "runtime_a2a3", "host", "a2a3_binary_loader.c"),
        os.path.join(RUNTIME_DIR, "runtime_a2a3", "core", "a2a3_core_worker.c"),
        os.path.join(RUNTIME_DIR, "runtime_a2a3", "orchestration", "a2a3_orchestration.c"),
    ]
    
    # Filter to only existing files
    runtime_sources = [s for s in runtime_sources if os.path.exists(s)]
    
    if runtime_sources:
        # Build link flags
        link_flags = ["-lpthread", "-ldl"]
        if cann_available:
            # Add ACL runtime library when CANN SDK is available
            link_flags.append(f"-L{ascend_home}/lib64")
            link_flags.append("-lascendcl")
        
        cmd = (
            f"gcc {' '.join(compile_flags)} {' '.join(include_paths)} "
            f"-o {test_exe} {test_file} {' '.join(runtime_sources)} "
            f"{' '.join(link_flags)}"
        )
        
        print(f"    Compiling test program...")
        ok, stdout, stderr = run_command(cmd, cwd=code_dir, timeout=120)
        
        if ok:
            print(f"    ✓ Compiled: {test_exe}")
        else:
            print(f"    ✗ Failed: {stderr}")
            success = False
    else:
        print(f"    ✗ Runtime source files not found")
        success = False
    
    return success


# =============================================================================
# Task Dump and Statistics
# =============================================================================

def run_task_dump():
    """Run and collect task dump statistics."""
    if not CONFIG['enable_task_dump']:
        return True
    
    print_header("Task Dump & Statistics")
    
    platform = CONFIG['target_platform']
    platform_dir = os.path.join(OUTPUT_DIR, platform)
    
    # Find executable (must be a file, not directory)
    exe_file = None
    for f in os.listdir(platform_dir):
        if not f.endswith(('.c', '.cu', '.cpp', '.txt', '.pdf', '.json', '.h')):
            exe_path = os.path.join(platform_dir, f)
            if os.path.isfile(exe_path) and os.access(exe_path, os.X_OK):
                exe_file = exe_path
                break
    
    if not exe_file:
        print("  No executable found")
        return False
    
    print(f"  Running: {os.path.basename(exe_file)}")
    success, stdout, stderr = run_command(exe_file, cwd=platform_dir, timeout=60)
    
    if success:
        print("  Execution successful")
        if stdout:
            print("  Output:")
            for line in stdout.split('\n')[:20]:
                print(f"    {line}")
        
        # Look for dump file
        dump_file = exe_file.replace('_orchestration', '_task_graph') + '.txt'
        if os.path.exists(dump_file):
            print(f"\n  Task graph dump: {dump_file}")
            analyze_task_dump(dump_file)
    else:
        print(f"  Execution failed: {stderr}")
    
    return success


def analyze_task_dump(dump_file):
    """Analyze task dump file and print statistics."""
    print("\n  Task Dump Statistics:")
    print("  " + "-" * 40)
    
    try:
        with open(dump_file, 'r') as f:
            content = f.read()
        
        # Count tasks
        task_count = content.count("Task ")
        print(f"    Total tasks: {task_count}")
        
        # Count by type if available
        lines = content.split('\n')
        task_types = dict()
        for line in lines:
            if "func=" in line:
                # Extract function name
                start = line.find("func=") + 5
                end = line.find(",", start) if "," in line[start:] else len(line)
                func_name = line[start:end].strip('"')
                task_types[func_name] = task_types.get(func_name, 0) + 1
        
        if task_types:
            print("    Tasks by function:")
            for func, count in sorted(task_types.items(), key=lambda x: -x[1]):
                print(f"      {func}: {count}")
        
        # Dependency info
        dep_count = content.count("fanin=")
        print(f"    Dependencies tracked: {dep_count}")
        
    except Exception as e:
        print(f"    Error analyzing dump: {e}")


# =============================================================================
# Task Graph PDF Generation
# =============================================================================

def generate_task_graph_pdf():
    """Generate task graph visualization as PDF."""
    if not CONFIG['enable_task_graph_pdf']:
        return True
    
    print_header("Task Graph PDF Generation")
    
    # Check if graphviz is available
    success, _, _ = run_command("which dot")
    if not success:
        print("  Warning: graphviz not installed, skipping PDF generation")
        return True
    
    platform = CONFIG['target_platform']
    platform_dir = os.path.join(OUTPUT_DIR, platform)
    
    # Look for task graph txt file
    txt_file = None
    for f in os.listdir(platform_dir):
        if 'task_graph' in f and f.endswith('.txt'):
            txt_file = os.path.join(platform_dir, f)
            break
    
    if not txt_file:
        print("  No task graph file found")
        return True
    
    # Try to use visualize_taskgraph.py if available (in scripts/ directory)
    vis_script = os.path.join(ROOT_DIR, "scripts", "visualize_taskgraph.py")
    if os.path.exists(vis_script):
        print(f"  Using visualize_taskgraph.py")
        cmd = f"python3 {vis_script} {txt_file}"
        success, stdout, stderr = run_command(cmd)
        if success:
            pdf_file = txt_file.replace('.txt', '.pdf')
            print(f"  Generated: {pdf_file}")
        else:
            print(f"  Warning: PDF generation failed: {stderr}")
    else:
        print("  visualize_taskgraph.py not found at {vis_script}")
    
    return True


# =============================================================================
# Performance Benchmark
# =============================================================================

def run_performance_benchmark():
    """Run performance benchmarks based on configuration."""
    success = True
    
    # Run orchestration benchmark if enabled
    if CONFIG.get('benchmark_orchestration', False):
        if not run_orchestration_benchmark():
            success = False
    
    # Run runtime benchmark if enabled
    if CONFIG.get('benchmark_runtime', False):
        if not run_runtime_benchmark():
            success = False
    
    # If neither benchmark enabled, just return True
    if not CONFIG.get('benchmark_orchestration', False) and not CONFIG.get('benchmark_runtime', False):
        return True
    
    return success


def run_orchestration_benchmark():
    """Orchestration Benchmark - measures task submission throughput (tasks/ms) without executing."""
    print_header("Orchestration Benchmark")
    print("  Measuring task submission throughput (tasks/ms) without executing tasks")
    
    TILE_ROWS = 32  # Must match pto_llama7B_dynamic.py
    
    platform = CONFIG['target_platform']
    platform_dir = os.path.join(OUTPUT_DIR, platform)
    
    # Find executable
    exe_file = find_executable(platform_dir)
    if not exe_file:
        print("  No executable found for benchmarking")
        return False
    
    print(f"  Executable: {os.path.basename(exe_file)}")
    print(f"  Seq length: {CONFIG['test_seq_len_min']} - {CONFIG['test_seq_len_max']} tokens (step: {CONFIG['test_seq_len_step']})")
    print(f"  Iterations: {CONFIG['num_benchmark_iterations']}")
    
    all_results = []
    seq_lengths = list(range(CONFIG['test_seq_len_min'], CONFIG['test_seq_len_max'] + 1, CONFIG['test_seq_len_step']))
    
    print(f"\n  Testing {len(seq_lengths)} sequence lengths...\n")
    print("  " + "-" * 85)
    print(f"  {'seq_len':>10} | {'num_tiles':>10} | {'tasks':>10} | {'orch_time(ms)':>14} | {'tasks/ms':>12} | {'throughput':>15}")
    print("  " + "-" * 85)
    
    for seq_len in seq_lengths:
        num_tiles = seq_len // TILE_ROWS
        # Run with --benchmark-only flag (or environment variable)
        cmd = f"{exe_file} --benchmark-only 0 0 {num_tiles} 0"
        
        times = []
        tasks_submitted = 0
        tasks_per_ms_values = []
        
        for i in range(CONFIG['num_benchmark_iterations']):
            success, stdout, stderr = run_command(cmd, cwd=platform_dir, timeout=60)
            
            if success:
                import re
                # Parse BENCHMARK output: tasks=X time_ms=Y tasks_per_ms=Z
                match = re.search(r'BENCHMARK:.*tasks=(\d+)\s+time_ms=([\d.]+)\s+tasks_per_ms=([\d.]+)', stdout)
                if match:
                    tasks_submitted = int(match.group(1))
                    time_ms = float(match.group(2))
                    tpm = float(match.group(3))
                    times.append(time_ms)
                    tasks_per_ms_values.append(tpm)
        
        if times and tasks_per_ms_values:
            avg_time = sum(times) / len(times)
            avg_tpm = sum(tasks_per_ms_values) / len(tasks_per_ms_values)
            throughput = f"{avg_tpm * 1000:.0f} tasks/s"
            
            print(f"  {seq_len:>10} | {num_tiles:>10} | {tasks_submitted:>10} | {avg_time:>14.3f} | {avg_tpm:>12.2f} | {throughput:>15}")
            
            all_results.append({
                "seq_len": seq_len,
                "num_tiles": num_tiles,
                "tasks_submitted": tasks_submitted,
                "avg_time_ms": avg_time,
                "min_time_ms": min(times),
                "max_time_ms": max(times),
                "tasks_per_ms": avg_tpm,
                "tasks_per_sec": avg_tpm * 1000,
            })
        else:
            print(f"  {seq_len:>10} | {num_tiles:>10} | {'FAILED':>10} | {'-':>14} | {'-':>12} | {'-':>15}")
    
    print("  " + "-" * 85)
    
    if all_results:
        save_benchmark_results(platform_dir, "orchestration", all_results)
    
    return True


def run_runtime_benchmark():
    """Runtime Benchmark - measures actual execution/simulation time with workers."""
    print_header("Runtime Benchmark")
    print("  Measuring actual execution time with workers")
    
    TILE_ROWS = 32  # Must match pto_llama7B_dynamic.py
    
    platform = CONFIG['target_platform']
    platform_dir = os.path.join(OUTPUT_DIR, platform)
    
    # Find executable
    exe_file = find_executable(platform_dir)
    if not exe_file:
        print("  No executable found for benchmarking")
        return False
    
    print(f"  Executable: {os.path.basename(exe_file)}")
    print(f"  Seq length: {CONFIG['test_seq_len_min']} - {CONFIG['test_seq_len_max']} tokens (step: {CONFIG['test_seq_len_step']})")
    print(f"  Iterations: {CONFIG['num_benchmark_iterations']}")
    
    all_results = []
    seq_lengths = list(range(CONFIG['test_seq_len_min'], CONFIG['test_seq_len_max'] + 1, CONFIG['test_seq_len_step']))
    
    print(f"\n  Testing {len(seq_lengths)} sequence lengths...\n")
    print("  " + "-" * 65)
    print(f"  {'seq_len':>10} | {'num_tiles':>10} | {'tasks':>10} | {'exec_time(ms)':>14} | {'tasks/ms':>12}")
    print("  " + "-" * 65)
    
    for seq_len in seq_lengths:
        num_tiles = seq_len // TILE_ROWS
        # Run full execution (no --benchmark-only flag)
        cmd = f"{exe_file} 0 0 {num_tiles} 0"
        
        times = []
        tasks_submitted = 0
        
        for i in range(CONFIG['num_benchmark_iterations']):
            start = time.perf_counter()
            success, stdout, stderr = run_command(cmd, cwd=platform_dir, timeout=300)
            end = time.perf_counter()
            
            if success:
                elapsed_ms = (end - start) * 1000
                times.append(elapsed_ms)
                
                import re
                # Parse tasks submitted
                match = re.search(r'Submitted (\d+) tasks', stdout)
                if match:
                    tasks_submitted = int(match.group(1))
        
        if times:
            avg_time = sum(times) / len(times)
            tasks_per_ms = tasks_submitted / avg_time if avg_time > 0 else 0
            
            print(f"  {seq_len:>10} | {num_tiles:>10} | {tasks_submitted:>10} | {avg_time:>14.2f} | {tasks_per_ms:>12.2f}")
            
            all_results.append({
                "seq_len": seq_len,
                "num_tiles": num_tiles,
                "tasks_submitted": tasks_submitted,
                "avg_time_ms": avg_time,
                "min_time_ms": min(times),
                "max_time_ms": max(times),
                "tasks_per_ms": tasks_per_ms,
            })
        else:
            print(f"  {seq_len:>10} | {num_tiles:>10} | {'FAILED':>10} | {'-':>14} | {'-':>12}")
    
    print("  " + "-" * 65)
    
    if all_results:
        save_benchmark_results(platform_dir, "runtime", all_results)
    
    return True


def find_executable(platform_dir):
    """Find executable in platform directory."""
    for f in os.listdir(platform_dir):
        if f.endswith(('.c', '.cu', '.cpp', '.txt', '.pdf', '.json', '.h')):
            continue
        exe_path = os.path.join(platform_dir, f)
        if os.path.isfile(exe_path) and os.access(exe_path, os.X_OK):
            return exe_path
    return None


def save_benchmark_results(platform_dir, benchmark_type, all_results):
    """Save benchmark results and print summary."""
    if benchmark_type == "orchestration":
        avg_tpm = sum(r['tasks_per_ms'] for r in all_results) / len(all_results)
        max_tpm = max(r['tasks_per_ms'] for r in all_results)
        min_tpm = min(r['tasks_per_ms'] for r in all_results)
        
        print(f"\n  Summary:")
        print(f"    Average: {avg_tpm:.2f} tasks/ms ({avg_tpm * 1000:.0f} tasks/s)")
        print(f"    Peak:    {max_tpm:.2f} tasks/ms ({max_tpm * 1000:.0f} tasks/s)")
        print(f"    Min:     {min_tpm:.2f} tasks/ms ({min_tpm * 1000:.0f} tasks/s)")
        
        summary = {
            "avg_tasks_per_ms": avg_tpm,
            "max_tasks_per_ms": max_tpm,
            "min_tasks_per_ms": min_tpm,
        }
    else:
        avg_time = sum(r['avg_time_ms'] for r in all_results) / len(all_results)
        avg_tasks_per_ms = sum(r.get('tasks_per_ms', 0) for r in all_results) / len(all_results)
        
        print(f"\n  Summary:")
        print(f"    Average execution time: {avg_time:.2f} ms")
        print(f"    Average throughput: {avg_tasks_per_ms:.2f} tasks/ms")
        
        summary = {
            "avg_execution_time_ms": avg_time,
            "avg_tasks_per_ms": avg_tasks_per_ms,
        }
    
    results = {
        "timestamp": datetime.now().isoformat(),
        "benchmark_type": benchmark_type,
        "platform": CONFIG['target_platform'],
        "seq_len_range": {
            "min": CONFIG['test_seq_len_min'],
            "max": CONFIG['test_seq_len_max'],
            "step": CONFIG['test_seq_len_step']
        },
        "iterations": CONFIG['num_benchmark_iterations'],
        "results": all_results,
        "summary": summary,
    }
    
    results_file = os.path.join(platform_dir, f"benchmark_{benchmark_type}.json")
    with open(results_file, 'w') as f:
        json.dump(results, f, indent=2)
    print(f"\n  Results saved to: {results_file}")




# =============================================================================
# Accuracy Test
# =============================================================================

def run_accuracy_test():
    """Generate and run accuracy tests using Python reference implementation."""
    if not CONFIG['enable_accuracy_test']:
        return True
    
    print_header("Accuracy Test")
    
    platform_dir = os.path.join(OUTPUT_DIR, CONFIG['target_platform'])
    
    # Check if accuracy data files exist
    input_file = os.path.join(platform_dir, "accuracy_input.bin")
    output_file = os.path.join(platform_dir, "accuracy_output.bin")
    params_file = os.path.join(platform_dir, "accuracy_params.txt")
    
    if not os.path.exists(input_file) or not os.path.exists(output_file):
        print("  Accuracy data files not found.")
        print("  Run test_program first to generate accuracy_input.bin and accuracy_output.bin")
        return True
    
    try:
        import numpy as np
    except ImportError:
        print("  NumPy not available. Skipping accuracy test.")
        return True
    
    # Read parameters
    params = dict()
    if os.path.exists(params_file):
        with open(params_file, 'r') as f:
            for line in f:
                line = line.strip()
                if '=' in line:
                    key, val = line.split('=', 1)
                    params[key] = int(val)
    
    # Default BGEMM parameters
    num_tiles = params.get('num_tiles', 8)
    tile_m = params.get('tile_m', 64)
    tile_n = params.get('tile_n', 128)
    tile_k = params.get('tile_k', 64)
    k_tiles = params.get('k_tiles', 8)
    
    print(f"  Parameters: num_tiles={num_tiles}, tile_m={tile_m}, tile_n={tile_n}, tile_k={tile_k}, k_tiles={k_tiles}")
    
    # Read input and output data
    input_data = np.fromfile(input_file, dtype=np.float32)
    output_data = np.fromfile(output_file, dtype=np.float32)
    
    print(f"  Input size: {len(input_data)} floats")
    print(f"  Output size: {len(output_data)} floats")
    
    # Split input into A and B matrices (same as test_program.c)
    half_size = len(input_data) // 2
    A_flat = input_data[:half_size]
    B_flat = input_data[half_size:]
    
    # Compute expected output using Python reference
    # BGEMM: For each output tile, C[tile] = sum_k(A[tile*k_tiles+k] @ B[k*num_tiles+tile])
    print("  Computing Python reference...")
    
    tile_size = tile_m * tile_n
    expected_output = np.zeros(num_tiles * tile_size, dtype=np.float32)
    
    for tile in range(num_tiles):
        # Accumulate partial products for this tile
        C_tile = np.zeros((tile_m, tile_n), dtype=np.float32)
        
        for k in range(k_tiles):
            # Get A tile: A[tile * k_tiles + k]
            a_idx = (tile * k_tiles + k) * tile_m * tile_k
            if a_idx + tile_m * tile_k <= len(A_flat):
                A_tile = A_flat[a_idx : a_idx + tile_m * tile_k].reshape(tile_m, tile_k)
            else:
                A_tile = np.zeros((tile_m, tile_k), dtype=np.float32)
            
            # Get B tile: B[k * num_tiles + tile]
            b_idx = (k * num_tiles + tile) * tile_k * tile_n
            if b_idx + tile_k * tile_n <= len(B_flat):
                B_tile = B_flat[b_idx : b_idx + tile_k * tile_n].reshape(tile_k, tile_n)
            else:
                B_tile = np.zeros((tile_k, tile_n), dtype=np.float32)
            
            # Accumulate: C += A @ B
            C_tile += np.matmul(A_tile, B_tile)
        
        # Store result
        expected_output[tile * tile_size : (tile + 1) * tile_size] = C_tile.flatten()
    
    # Compare results
    output_elements = num_tiles * tile_size
    actual = output_data[:output_elements]
    expected = expected_output[:output_elements]
    
    # Compute differences
    diff = np.abs(expected - actual)
    max_diff = np.max(diff)
    max_diff_idx = np.argmax(diff)
    
    # Use relative tolerance for floating point
    rtol = 1e-3  # Relative tolerance
    atol = 1e-5  # Absolute tolerance
    
    # Check for errors
    errors = np.sum(diff > (atol + rtol * np.abs(expected)))
    
    print(f"  Max difference: {max_diff:.6f} at index {max_diff_idx}")
    print(f"  Expected[{max_diff_idx}]: {expected[max_diff_idx]:.6f}")
    print(f"  Actual[{max_diff_idx}]: {actual[max_diff_idx]:.6f}")
    
    if errors > 0:
        # Show first few mismatches
        mismatch_indices = np.where(diff > (atol + rtol * np.abs(expected)))[0][:10]
        for idx in mismatch_indices:
            print(f"    Mismatch at [{idx}]: expected {expected[idx]:.6f}, got {actual[idx]:.6f}")
    
    print()
    print("=" * 60)
    if errors == 0:
        print("  ACCURACY TEST: PASSED")
        result = True
    else:
        print(f"  ACCURACY TEST: FAILED ({errors} errors out of {output_elements})")
        result = False
    print("=" * 60)
    
    return result


# =============================================================================
# Simulation and Trace Generation
# =============================================================================

def run_simulation():
    """Run simulation and generate trace files."""
    if not CONFIG['enable_simulation']:
        return True
    
    print_header("Simulation & Trace Generation")
    
    if CONFIG['target_platform'] != 'ascend_a2a3_sim':
        print("  Simulation only available for ascend_a2a3_sim platform")
        return True
    
    platform_dir = os.path.join(OUTPUT_DIR, CONFIG['target_platform'])
    
    # Find simulation executable (any executable that's not a known non-executable extension)
    exe_file = None
    for f in os.listdir(platform_dir):
        # Skip directories, source files, and known output files
        if f.endswith(('.c', '.cu', '.cpp', '.txt', '.pdf', '.json', '.h')):
            continue
        exe_path = os.path.join(platform_dir, f)
        # Must be a file (not directory) and executable
        if os.path.isfile(exe_path) and os.access(exe_path, os.X_OK):
            exe_file = exe_path
            break
    
    if not exe_file:
        # Simulation already ran during task dump, so this is not a failure
        print("  Note: Simulation already completed during task dump step")
        return True
    
    # Calculate num_tiles for simulation using max seq_len from config
    TILE_ROWS = 32  # Must match pto_llama7B_dynamic.py
    sim_seq_len = CONFIG['test_seq_len_max']
    sim_num_tiles = sim_seq_len // TILE_ROWS
    
    print(f"  Running simulation: {os.path.basename(exe_file)}")
    print(f"  Simulation parameters: seq_len={sim_seq_len}, num_tiles={sim_num_tiles}")
    
    # Run with trace enabled and proper parameters
    env = os.environ.copy()
    env['PTO_TRACE_OUTPUT'] = os.path.join(platform_dir, 'trace.json')
    
    # Pass seq_len, tile_rows, num_tiles, zero as arguments
    sim_cmd = f"{exe_file} {sim_seq_len} {TILE_ROWS} {sim_num_tiles} 0"
    success, stdout, stderr = run_command(sim_cmd, cwd=platform_dir, timeout=120)
    
    if success:
        trace_file = os.path.join(platform_dir, 'trace.json')
        if os.path.exists(trace_file):
            print(f"  Trace file generated: {trace_file}")
            
            # Basic trace analysis
            try:
                with open(trace_file, 'r') as f:
                    trace_data = json.load(f)
                if isinstance(trace_data, list):
                    print(f"  Trace events: {len(trace_data)}")
            except:
                pass
        else:
            print("  Note: Trace file not generated (may need runtime support)")
    else:
        print(f"  Simulation failed: {stderr}")
    
    return success


# =============================================================================
# Main
# =============================================================================

def main():
    print_header(f"PTO Example Runner: {CONFIG['example_name']}")
    print(f"  Platform: {CONFIG['target_platform']}")
    print(f"  Output:   {OUTPUT_DIR}")
    
    ensure_dir(OUTPUT_DIR)
    
    steps = [
        ("Code Generation", generate_code),
        ("Compilation", compile_code),
        ("Task Dump", run_task_dump),
        ("Task Graph PDF", generate_task_graph_pdf),
        ("Performance Benchmark", run_performance_benchmark),
        ("Accuracy Test", run_accuracy_test),
        ("Simulation", run_simulation),
    ]
    
    results = []
    for name, func in steps:
        try:
            success = func()
            results.append((name, success))
        except Exception as e:
            print(f"  Error in {name}: {e}")
            results.append((name, False))
    
    print_header("Summary")
    for name, success in results:
        status = "✓ OK" if success else "✗ FAILED"
        print(f"  {name}: {status}")
    
    print("\nDone!")


if __name__ == "__main__":
    main()
